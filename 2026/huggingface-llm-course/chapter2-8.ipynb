{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8efec585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: openai in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: vllm in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (0.14.1)\n",
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (6.0.3)\n",
      "Requirement already satisfied: requests in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from openai) (4.12.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from openai) (2.12.5)\n",
      "Requirement already satisfied: sniffio in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
      "Requirement already satisfied: certifi in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
      "Requirement already satisfied: regex in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (2026.1.15)\n",
      "Requirement already satisfied: cachetools in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (7.0.0)\n",
      "Requirement already satisfied: psutil in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (7.0.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.2.1)\n",
      "Requirement already satisfied: numpy in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (2.2.6)\n",
      "Requirement already satisfied: blake3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.0.8)\n",
      "Requirement already satisfied: py-cpuinfo in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (9.0.0)\n",
      "Requirement already satisfied: transformers<5,>=4.56.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (4.57.6)\n",
      "Requirement already satisfied: tokenizers>=0.21.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.22.2)\n",
      "Requirement already satisfied: protobuf>=6.30.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (6.33.5)\n",
      "Requirement already satisfied: fastapi>=0.115.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.128.5)\n",
      "Requirement already satisfied: aiohttp in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (3.13.3)\n",
      "Requirement already satisfied: prometheus_client>=0.18.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.24.1)\n",
      "Requirement already satisfied: pillow in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (12.1.0)\n",
      "Requirement already satisfied: prometheus-fastapi-instrumentator>=7.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (7.1.0)\n",
      "Requirement already satisfied: tiktoken>=0.6.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.12.0)\n",
      "Requirement already satisfied: lm-format-enforcer==0.11.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.11.3)\n",
      "Requirement already satisfied: llguidance<1.4.0,>=1.3.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.3.0)\n",
      "Requirement already satisfied: outlines_core==0.2.11 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.2.11)\n",
      "Requirement already satisfied: diskcache==5.6.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (5.6.3)\n",
      "Requirement already satisfied: lark==1.2.2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.2.2)\n",
      "Requirement already satisfied: xgrammar==0.1.29 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.1.29)\n",
      "Requirement already satisfied: partial-json-parser in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.2.1.1.post7)\n",
      "Requirement already satisfied: pyzmq>=25.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (27.1.0)\n",
      "Requirement already satisfied: msgspec in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: gguf>=0.17.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.17.1)\n",
      "Requirement already satisfied: mistral_common>=1.8.8 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from mistral_common[image]>=1.8.8->vllm) (1.9.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.13.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (4.13.0.92)\n",
      "Requirement already satisfied: six>=1.16.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.17.0)\n",
      "Requirement already satisfied: setuptools<81.0.0,>=77.0.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (77.0.3)\n",
      "Requirement already satisfied: einops in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.8.2)\n",
      "Requirement already satisfied: compressed-tensors==0.13.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.13.0)\n",
      "Requirement already satisfied: depyf==0.20.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.20.0)\n",
      "Requirement already satisfied: cloudpickle in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (3.1.2)\n",
      "Requirement already satisfied: watchfiles in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.1.1)\n",
      "Requirement already satisfied: python-json-logger in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (4.0.0)\n",
      "Requirement already satisfied: ninja in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.13.0)\n",
      "Requirement already satisfied: pybase64 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.4.3)\n",
      "Requirement already satisfied: cbor2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (5.8.0)\n",
      "Requirement already satisfied: ijson in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (3.4.0.post0)\n",
      "Requirement already satisfied: setproctitle in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.3.7)\n",
      "Requirement already satisfied: openai-harmony>=0.0.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.0.8)\n",
      "Requirement already satisfied: anthropic==0.71.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.71.0)\n",
      "Requirement already satisfied: model-hosting-container-standards<1.0.0,>=0.1.10 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.1.13)\n",
      "Requirement already satisfied: mcp in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.26.0)\n",
      "Requirement already satisfied: grpcio>=1.76.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.78.0)\n",
      "Requirement already satisfied: grpcio-reflection>=1.76.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (1.78.0)\n",
      "Requirement already satisfied: numba==0.61.2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.61.2)\n",
      "Requirement already satisfied: torch==2.9.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (2.9.1)\n",
      "Requirement already satisfied: torchaudio in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (2.9.1)\n",
      "Requirement already satisfied: torchvision in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from vllm) (0.24.1)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.15 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from anthropic==0.71.0->vllm) (0.17.0)\n",
      "Requirement already satisfied: loguru in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from compressed-tensors==0.13.0->vllm) (0.7.3)\n",
      "Requirement already satisfied: astor in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from depyf==0.20.0->vllm) (0.8.1)\n",
      "Requirement already satisfied: dill in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from depyf==0.20.0->vllm) (0.4.0)\n",
      "Requirement already satisfied: interegular>=0.3.2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from lm-format-enforcer==0.11.3->vllm) (0.3.3)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from numba==0.61.2->vllm) (0.44.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from torch==2.9.1->vllm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from torch==2.9.1->vllm) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from torch==2.9.1->vllm) (3.1.6)\n",
      "Requirement already satisfied: mlx-lm in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from xgrammar==0.1.29->vllm) (0.29.1)\n",
      "Requirement already satisfied: jmespath in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (1.1.0)\n",
      "Requirement already satisfied: starlette>=0.49.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (0.52.1)\n",
      "Requirement already satisfied: supervisor>=4.2.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from model-hosting-container-standards<1.0.0,>=0.1.10->vllm) (4.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from transformers<5,>=4.56.0->vllm) (0.7.0)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi>=0.115.0->fastapi[standard]>=0.115.0->vllm) (0.0.4)\n",
      "Requirement already satisfied: fastapi-cli>=0.0.8 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.0.20)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (0.0.22)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.3.0)\n",
      "Requirement already satisfied: uvicorn>=0.12.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.40.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.12.0)\n",
      "Requirement already satisfied: pydantic-extra-types>=2.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi[standard]>=0.115.0->vllm) (2.11.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from email-validator>=2.0.0->fastapi[standard]>=0.115.0->vllm) (2.8.0)\n",
      "Requirement already satisfied: typer>=0.15.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.21.1)\n",
      "Requirement already satisfied: rich-toolkit>=0.14.8 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.18.1)\n",
      "Requirement already satisfied: fastapi-cloud-cli>=0.1.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.11.0)\n",
      "Requirement already satisfied: rignore>=0.5.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.20.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.52.0)\n",
      "Requirement already satisfied: fastar>=0.8.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from fastapi-cloud-cli>=0.1.1->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.8.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from jinja2->torch==2.9.1->vllm) (3.0.3)\n",
      "Requirement already satisfied: jsonschema>=4.21.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (4.26.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (25.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.25.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from jsonschema>=4.21.1->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (0.30.0)\n",
      "Requirement already satisfied: pycountry>=23 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pydantic-extra-types[pycountry]>=2.10.5->mistral_common>=1.8.8->mistral_common[image]>=1.8.8->vllm) (24.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pydantic-settings>=2.0.0->fastapi[standard]>=0.115.0->vllm) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from requests->huggingface_hub) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from requests->huggingface_hub) (2.6.3)\n",
      "Requirement already satisfied: click>=8.1.7 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (8.3.1)\n",
      "Requirement already satisfied: rich>=13.7.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (14.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->rich-toolkit>=0.14.8->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.1.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.9.1->vllm) (1.3.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from typer>=0.15.1->fastapi-cli>=0.0.8->fastapi-cli[standard]>=0.0.8; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (0.22.1)\n",
      "Requirement already satisfied: websockets>=10.4 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from uvicorn[standard]>=0.12.0; extra == \"standard\"->fastapi[standard]>=0.115.0->vllm) (16.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from aiohttp->vllm) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from aiohttp->vllm) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from aiohttp->vllm) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from aiohttp->vllm) (1.22.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from mcp->vllm) (0.4.3)\n",
      "Requirement already satisfied: pyjwt>=2.10.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (2.11.0)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from mcp->vllm) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from pyjwt[crypto]>=2.10.1->mcp->vllm) (46.0.4)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography>=3.4.0->pyjwt[crypto]>=2.10.1->mcp->vllm) (3.0)\n",
      "Requirement already satisfied: mlx>=0.29.2 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from mlx-lm->xgrammar==0.1.29->vllm) (0.30.6)\n",
      "Requirement already satisfied: mlx-metal==0.30.6 in /Users/danielwohlgemuth/Code/experiments/.conda/lib/python3.12/site-packages (from mlx>=0.29.2->mlx-lm->xgrammar==0.1.29->vllm) (0.30.6)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-macosx_15_0_arm64.whl size=3848476 sha256=452fc645b012c36d7e9016fc56a96a8526d509cc520f82253b754449ba6a69f3\n",
      "  Stored in directory: /Users/danielwohlgemuth/Library/Caches/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: llama-cpp-python\n",
      "Successfully installed llama-cpp-python-0.3.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install huggingface_hub openai vllm llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d18f1",
   "metadata": {},
   "source": [
    "## TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999a46b9",
   "metadata": {},
   "source": [
    "Launch the TGI server using Docker. Note: There is no support for Macbook M series.\n",
    "\n",
    "```bash\n",
    "docker run --gpus all \\\n",
    "    --shm-size 1g \\\n",
    "    -p 8080:80 \\\n",
    "    -v ~/.cache/huggingface:/data\\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
    "    --device 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba17d674",
   "metadata": {},
   "source": [
    "Interact with TGI using the Hugging Face Inference Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedebe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080\",\n",
    ")\n",
    "\n",
    "response = client.text_generation(\n",
    "    \"Tell me a story\",\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    "    stop_sequences=[],\n",
    ")\n",
    "print(response.generated_text)\n",
    "\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": \"Tell me a story\" },\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf196ae",
   "metadata": {},
   "source": [
    "Interact with TGI using the OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c58bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080\",\n",
    "    api_key=\"not-needed\",\n",
    ")\n",
    "\n",
    "response = client.chat.completion.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    message=[\n",
    "        { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "        { \"role\": \"user\", \"content\": \"Tell a story\" },\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].messages.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb970e9f",
   "metadata": {},
   "source": [
    "## llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2b85d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/ggerganov/llama.cpp --depth 1\n",
    "cd llama.cpp\n",
    "\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j 8\n",
    "\n",
    "curl -L -O \"https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct-GGUF/resolve/main/smollm2-1.7b-instruct-q4_k_m.gguf?download=true\" -o ./models/smollm2-1.7b-instruct-q4_k_m.gguf\n",
    "\n",
    "./build/bin/llama-server \\\n",
    "    -m ./models/smollm2-1.7b-instruct-q4_k_m.gguf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080 \\\n",
    "    -c 4096 \\\n",
    "    --n-gpu-layers 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3d78c",
   "metadata": {},
   "source": [
    "Alternative using Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1331fd",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run -v ./models:/models -p 8080:8080 ghcr.io/ggml-org/llama.cpp:server -m /models/smollm2-1.7b-instruct-q4_k_m.gguf --port 8080 --host 0.0.0.0 -n 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c30ca37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the land of Aethoria, where the sun dipped into the horizon and painted the sky with hues of crimson and gold, there was a village known for its magical creatures. Among them was a young fawn named Luna, who possessed an extraordinary gift – she was the only one in Aethoria who could communicate with the magical animals.\n",
      "\n",
      "Luna lived with her wise old mother, a talking owl named Orion, in a cozy little hut surrounded by a lush forest filled with the\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize client pointing to llama.cpp server\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8080/v1\",  # URL to the llama.cpp server\n",
    "    token=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
    ")\n",
    "\n",
    "# Text generation\n",
    "# response = client.text_generation(\n",
    "#     \"Tell me a story\",\n",
    "#     max_new_tokens=100,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     details=True,\n",
    "# )\n",
    "# print(response.generated_text)\n",
    "\n",
    "# For chat format\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7fb056fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a world of endless possibilities, there was a young girl named Luna. She was a curious and adventurous girl, always eager to explore the world around her. She lived in a small village at the edge of a vast and mysterious forest.\n",
      "\n",
      "One day, Luna decided that she wanted to explore the forest. She packed her bag with food, water, and a map of the forest. Her parents were a bit worried, but they trusted her to be safe.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client pointing to llama.cpp server\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8080/v1\",\n",
    "    api_key=\"sk-no-key-required\",  # llama.cpp server requires this placeholder\n",
    ")\n",
    "\n",
    "# Chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",  # Model identifier can be anything as server only loads one model\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22bcbd6",
   "metadata": {},
   "source": [
    "## vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aa857",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install vllm\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "    --model HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83e36763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a faraway land called Azura, there lived a wise and powerful wizard named Eryndor Thorne. Eryndor was a master of ancient magic and had spent his life studying the mystical forces of the universe. He was known for his extraordinary abilities and was often sought after by those who sought to learn from his vast knowledge.\n",
      "\n",
      "One day, Eryndor was summoned by the kingdom of Everia, a land of beautiful gardens and prosperous\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Initialize client pointing to vLLM endpoint\n",
    "client = InferenceClient(\n",
    "    model=\"http://localhost:8000/v1\",  # URL to the vLLM server\n",
    ")\n",
    "\n",
    "# Text generation\n",
    "# response = client.text_generation(\n",
    "#     \"Tell me a story\",\n",
    "#     max_new_tokens=100,\n",
    "#     temperature=0.7,\n",
    "#     top_p=0.95,\n",
    "#     details=True,\n",
    "# )\n",
    "# print(response.generated_text)\n",
    "\n",
    "# For chat format\n",
    "response = client.chat_completion(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f226a8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a land far away, there lived a wise and powerful wizard named Thorold. Thorold was the king of the land, and his power was feared by all who lived there. He had a staff that could read the thoughts of those around him and could cast spells of destruction.\n",
      "\n",
      "One day, Thorold was at a great feast in the castle. He was surrounded by a group of his most trusted advisors, including his closest friend and confidant,\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client pointing to vLLM endpoint\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    "    api_key=\"not-needed\",  # vLLM doesn't require an API key by default\n",
    ")\n",
    "\n",
    "# Chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a story\"},\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125727ce",
   "metadata": {},
   "source": [
    "TGI with advanced paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d42823a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker run --gpus all \\\n",
    "    --shm-size 1g \\\n",
    "    -p 8080:80 \\\n",
    "    -v ~/.cache/huggingface:/data \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-360M-Instruct \\\n",
    "    --max-total-tokens 4096 \\\n",
    "    --max-input-length 3072 \\\n",
    "    --max-batch-total-tokens 8192 \\\n",
    "    --waiting-served-ratio 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fb4def",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"http://localhost:8080\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Raw text generation\n",
    "response = client.text_generation(\n",
    "    \"Write a creative story about space exploration\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.1,\n",
    "    do_sample=True,\n",
    "    details=True,\n",
    ")\n",
    "print(response.generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586d5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"not-needed\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,  # Higher for more creativity\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e9bc6",
   "metadata": {},
   "source": [
    "llama.cpp with advanced parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baed833",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./build/bin/llama-server \\\n",
    "    -m ./models/smollm2-1.7b-instruct-q4_k_m.gguf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080 \\\n",
    "    -c 4096 \\\n",
    "    --n-gpu-layers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1b8b327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in a far-off land, there lived a young girl named Luna. She was an adventurous girl, always seeking new experiences and learning from her surroundings. Luna lived in a quaint little village by the sea, where the salty winds and the salty sea air filled her lungs, and the salty sea water danced on her skin.\n",
      "\n",
      "One day, while Luna was out on her favorite spot on the beach, a sudden gust of wind swept in and blew her golden-blonde hair into a tangled mess. Luna was frantically trying to untangle her hair, but it seemed impossible. She was so frustrated that she decided to go home and get a hair cut, but before she left, she decided to ask her grandmother for help.\n",
      "\n",
      "\"Grandma, can you help me untangle my hair?\" Luna asked, her voice filled with excitement.\n",
      "\n",
      "Her grandmother, an old lady with a kind heart and a beautiful silver hair, smiled and nodded. She\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"http://localhost:8080/v1\", token=\"sk-no-key-required\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# For direct text generation\n",
    "# response = client.text_generation(\n",
    "#     \"Write a creative story about space exploration\",\n",
    "#     max_new_tokens=200,\n",
    "#     temperature=0.8,\n",
    "#     top_p=0.95,\n",
    "#     repetition_penalty=1.1,\n",
    "#     details=True,\n",
    "# )\n",
    "# print(response.generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b5e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in the land of Fantasia, there was a mystical creature named Luna. She was a being with the essence of both moon and stars within her. Luna was known for her enchanting beauty and divine powers that she used to bring peace to the warring creatures of Fantasia.\n",
      "\n",
      "Luna had an extraordinary ability - she could transform into any celestial body or any shape she wished, which made her one of the most feared as well as revered beings in Fantasia. She could morph into a shooting star, a bright moonlight, a radiant sunbeam or even a majestic starry sky at will.\n",
      "\n",
      "Luna was adored by all the creatures for her enchanting beauty and divine powers. They believed that she had magical powers to heal any ailment, bring good fortune and prosperity, and even grant wishes. Luna was kind-hearted and always used her powers for the good of the world.\n",
      "\n",
      "One day, a wicked witch named Malice sought to capture Luna\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,  # Higher for more creativity\n",
    "    top_p=0.95,  # Nucleus sampling probability\n",
    "    frequency_penalty=0.5,  # Reduce repetition of frequent tokens\n",
    "    presence_penalty=0.5,  # Reduce repetition by penalizing tokens already present\n",
    "    max_tokens=200,  # Maximum generation length\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19bc0d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, in the land of Imagination, there was an enchanted forest filled with magical creatures and talking animals. Among them were two best friends, Luna the Luminous Fairy and Orion the Orangutan. They lived in harmony, exploring every nook and cranny of their enchanting home.\n",
      "\n",
      "One day, while wandering through a particularly dense part of the forest, they stumbled upon an ancient tree with glowing leaves that shimmered like stars in the night sky. Luna was immediately\n",
      "Once upon a time, in a land far, far away, there was a magical kingdom known as the Land of Whimsy. This kingdom was filled with fantastical creatures, enchanted forests, and hidden treasures. Among the most magical of all the creatures in this kingdom was a young girl named Luna. Luna was a dreamer, always chasing her dreams and always believing in the magic that lay just beyond the edge of reality.\n",
      "\n",
      "Luna lived in a small cottage on the outskirts of the\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"sk-no-key-required\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.1,  # Higher for more creativity\n",
    "    top_p=0.95,  # Nucleus sampling probability\n",
    "    frequency_penalty=0.9,  # Reduce repetition of frequent tokens\n",
    "    presence_penalty=0.9,  # Reduce repetition by penalizing tokens already present\n",
    "    max_tokens=100,  # Maximum generation length\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.1,  # Higher for more creativity\n",
    "    top_p=0.95,  # Nucleus sampling probability\n",
    "    frequency_penalty=0.1,  # Reduce repetition of frequent tokens\n",
    "    presence_penalty=0.1,  # Reduce repetition by penalizing tokens already present\n",
    "    max_tokens=100,  # Maximum generation length\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96285212",
   "metadata": {},
   "source": [
    "Using llama.cpp's Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06c0b646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 34 key-value pairs and 218 tensors from llama.cpp/models/smollm2-1.7b-instruct-q4_k_m.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Smollm2 1.7B 8k Mix7 Ep2 v2\n",
      "llama_model_loader: - kv   3:                            general.version str              = v2\n",
      "llama_model_loader: - kv   4:                       general.organization str              = Loubnabnl\n",
      "llama_model_loader: - kv   5:                           general.finetune str              = 8k-mix7-ep2\n",
      "llama_model_loader: - kv   6:                           general.basename str              = smollm2\n",
      "llama_model_loader: - kv   7:                         general.size_label str              = 1.7B\n",
      "llama_model_loader: - kv   8:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"en\"]\n",
      "llama_model_loader: - kv  10:                          llama.block_count u32              = 24\n",
      "llama_model_loader: - kv  11:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                     llama.embedding_length u32              = 2048\n",
      "llama_model_loader: - kv  13:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  14:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  15:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  16:                       llama.rope.freq_base f32              = 130000.000000\n",
      "llama_model_loader: - kv  17:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 49152\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 64\n",
      "llama_model_loader: - kv  21:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = smollm\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,49152]   = [\"<|endoftext|>\", \"<|im_start|>\", \"<|...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,48900]   = [\"Ġ t\", \"Ġ a\", \"i n\", \"h e\", \"Ġ Ġ...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   49 tensors\n",
      "llama_model_loader: - type q4_K:  144 tensors\n",
      "llama_model_loader: - type q6_K:   25 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1005.01 MiB (4.93 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token:     15 '<jupyter_script>' is not marked as EOG\n",
      "load: control token:      7 '<gh_stars>' is not marked as EOG\n",
      "load: control token:      5 '<file_sep>' is not marked as EOG\n",
      "load: control token:     12 '<jupyter_text>' is not marked as EOG\n",
      "load: control token:     10 '<issue_closed>' is not marked as EOG\n",
      "load: control token:     11 '<jupyter_start>' is not marked as EOG\n",
      "load: control token:      1 '<|im_start|>' is not marked as EOG\n",
      "load: control token:      9 '<issue_comment>' is not marked as EOG\n",
      "load: control token:      8 '<issue_start>' is not marked as EOG\n",
      "load: control token:     13 '<jupyter_code>' is not marked as EOG\n",
      "load: control token:      3 '<repo_name>' is not marked as EOG\n",
      "load: control token:      6 '<filename>' is not marked as EOG\n",
      "load: control token:     14 '<jupyter_output>' is not marked as EOG\n",
      "load: control token:     16 '<empty_output>' is not marked as EOG\n",
      "load: printing all EOG tokens:\n",
      "load:   - 0 ('<|endoftext|>')\n",
      "load:   - 2 ('<|im_end|>')\n",
      "load:   - 4 ('<reponame>')\n",
      "load: special tokens cache size = 17\n",
      "load: token to piece cache size = 0.3170 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 2048\n",
      "print_info: n_layer          = 24\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 32\n",
      "print_info: n_rot            = 64\n",
      "print_info: n_swa            = 0\n",
      "print_info: is_swa_any       = 0\n",
      "print_info: n_embd_head_k    = 64\n",
      "print_info: n_embd_head_v    = 64\n",
      "print_info: n_gqa            = 1\n",
      "print_info: n_embd_k_gqa     = 2048\n",
      "print_info: n_embd_v_gqa     = 2048\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 8192\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 130000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = ?B\n",
      "print_info: model params     = 1.71 B\n",
      "print_info: general.name     = Smollm2 1.7B 8k Mix7 Ep2 v2\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 49152\n",
      "print_info: n_merges         = 48900\n",
      "print_info: BOS token        = 1 '<|im_start|>'\n",
      "print_info: EOS token        = 2 '<|im_end|>'\n",
      "print_info: EOT token        = 2 '<|im_end|>'\n",
      "print_info: UNK token        = 0 '<|endoftext|>'\n",
      "print_info: PAD token        = 2 '<|im_end|>'\n",
      "print_info: LF token         = 198 'Ċ'\n",
      "print_info: FIM REP token    = 4 '<reponame>'\n",
      "print_info: EOG token        = 0 '<|endoftext|>'\n",
      "print_info: EOG token        = 2 '<|im_end|>'\n",
      "print_info: EOG token        = 4 '<reponame>'\n",
      "print_info: max token length = 162\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 218 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors: offloading 0 repeating layers to GPU\n",
      "load_tensors: offloaded 0/25 layers to GPU\n",
      "load_tensors:   CPU_Mapped model buffer size =  1005.01 MiB\n",
      "........................................................................................\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 4096\n",
      "llama_context: n_ctx_per_seq = 4096\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: kv_unified    = false\n",
      "llama_context: freq_base     = 130000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (8192) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_load_library: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = true\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x1224ccae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_2                             0x1224ce6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_3                             0x1208ac2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_4                             0x1179ffa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_5                             0x1208ac5b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_6                             0x1179ffd10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_7                             0x1179fd750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_fuse_8                             0x1208ac940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4                             0x1179fda10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_2                      0x1224cd1d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_3                      0x1179fdcd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_4                      0x1208acdb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_5                      0x1208aefa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_6                      0x1208af9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_7                      0x1179fdf90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row_c4_fuse_8                      0x1179fe250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x1179fe610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row_c4                             0x1208ae660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x1179fe8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row_c4                             0x1179ff100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x130078420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row_c4                             0x1208ade70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_id                                 0x1208b1a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x1300786e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x130078b50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x1208b0a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x1224cdbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x1208b2330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x1208b3d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x1208b3700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x1208b5010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x1208b59c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x1208b61d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x1208b6930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x130079300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf                               0x1208b73a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_erf_4                             0x1208b7f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x1224cee70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x13007a1f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x13007a790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x1208b8ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x107021ea0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_abs                                    0x13007aa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sgn                                    0x1208ba550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_step                                   0x13007b140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardswish                              0x1224d04a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_hardsigmoid                            0x1224cff10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_exp                                    0x1208bb990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x1223fd2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x1208bb0c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x1224d0760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x1223fd670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x122030000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x117ec88f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x1224d0a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x122104430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x1220302c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x1224d0d30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x122030860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x122030d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x1208bccb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_mxfp4                         0x1220311e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x1220317a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x122032350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x1223fe500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x1208bd290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x1208bdff0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x122031da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x1223fe970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x1223fdae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x1208bd9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x1208bf6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x1223ff980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x122804b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x1228044a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x122032af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x122033600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f32                           0x122033f00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_f16                           0x1208be590 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_set_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_set_rows_q8_0                          0x1220347b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_0                          0x122034da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q4_1                          0x1224d2920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_0                          0x1224d3da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_q5_1                          0x1228058e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_rows_iq4_nl                        0x1208c0240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x122035a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul                           0x122036790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm_mul_add                       0x1208bf090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_l2_norm                                0x1208c17f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x1224d22e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x1224d4cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x122035060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x1208c0ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32_group                     0x1224d50a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv6_f32                          0x122806180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rwkv_wkv7_f32                          0x1224d56a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x1228066e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32_c4                      0x122035d50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_c4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x1208c22d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_c4                      0x122038dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x122037010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x1208c1bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x1208c3030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x1208c45b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x122807410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x1220390b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x122039370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x1208c4f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_mxfp4_f32                       0x12203a5f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x12203b1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x12203b460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x1224d7010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x1224d7de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x1208c6500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x1208c3cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x1224d87b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x1208c6ac0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x122806c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x1208c7340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x12203cfa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x12203bc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x1224d8e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x122808a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x1208c8380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x12203c980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x1208c8b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x1208c9040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x12203f070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x1224d9760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x1208c9690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x1208c9950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x12203ff00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x1208ca650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_2              0x12203e920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_3              0x1208ca9e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_4              0x1220408b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_mxfp4_f32_r1_5              0x122041a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x1208cb260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x1228097b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x1224da910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x1220423b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x12280a160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x12280abd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x12280b5d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x1224db010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x122042670 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x1208cc910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x1208cd310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x1224d6890 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x12280bfb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x122043190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x1208cc110 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x1208ce4e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x12203f9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x1208cdb10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x1208cddd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x1208cf8b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x1224dc260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x1208d0210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x1224dc5c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x1224dd6e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x122043c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x1208cf260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x1220434e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x122044d40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x1224de3a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x122046480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x12280cb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x122047190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x12280d540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x122808e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x12280e440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x122044750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x1220481f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_mxfp4_f32                    0x12280dc80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x1208d13b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x1224dd160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x122048ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x1224dfcf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x1208d28e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x1224df060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x122049580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x1224e0610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x1224e1460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x1208d3070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x1224e1eb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x1208d3330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x1208d4360 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x1208d3dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x1224e2730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x1208d55b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x12280efe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x122810280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x1224e2b80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x122049840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x1224e30e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x1208d5a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_mxfp4_f32                       0x12204a430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x122049b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x12204b100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x1224e3560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x1224e4a30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x1224e5a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x12204a9a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x1208d6780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x1208d7150 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x12204acb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x1208d75f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x1208d7e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x1208d8c60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x12204c240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x122811620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map0_f16                     0x1208d93c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_map1_f32                     0x12204d0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f16                      0x1224e5350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f16                      0x1208da070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f16                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f16                     0x1208db000 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f16                     0x1208da6d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f16                     0x1208dc1a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f16                     0x12204e770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f16                     0x12280f8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_mxfp4_f16                    0x1208dcb50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f16                     0x1208db950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f16                     0x1224e6620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f16                     0x1208dd540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f16                     0x1208ddbc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f16                     0x1208de270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f16                  0x1208de8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f16                   0x1208df7b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f16                  0x1224e7ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f16                    0x12204ee10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f16                    0x1224e6a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f16                    0x1224e8c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f16                    0x12204e2c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f16                   0x122050370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f16                   0x1208e0da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x12204faa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x122050710 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f32                         0x122051800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_multi_f16                         0x122051420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f32                        0x1208e0740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_vision_f16                        0x1208e2430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x1208e1c20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x1208e3380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x1208e3cc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x1208e46f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x1208e5060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x122051f30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x122053260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x1220529c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x1220543a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x122052c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x122054ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x1224e73a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x1208e40d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x122055e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x1220559c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x1208e6220 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x1208e64e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x1208e76f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x122057b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x1224ea3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x1208e8070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h192                0x1208e9730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk192_hv128         0x1228127e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x1208e9270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_hk576_hv512         0x1208ea880 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h192          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk192_hv128   (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_hk576_hv512   (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x1208eb330 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x122057420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x122814480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x1208ebba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x1224ea7d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h192               0x1208ec5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk192_hv128        0x1208eb720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x1208ed810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_hk576_hv512        0x1208ec960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x1220594d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x1208ed1e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x1228131f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x1228153e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x122814950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h192               0x122814db0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk192_hv128        0x1208edba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x1224eacb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_hk576_hv512        0x122059fc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x1208eee10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x122815e40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x12205a630 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x1208f0740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x12205bc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h192               0x1208f0140 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk192_hv128        0x1208ef3c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x1224ec700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_hk576_hv512        0x12205b2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x1208f10f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x122813540 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x1224ed3f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x12205d840 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x1208f2a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h192               0x12205c070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk192_hv128        0x1224ede10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x12205d230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_hk576_hv512        0x1208f2e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x1224ee7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x12205e830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x1208f32b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x12205f010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x1208f2490 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h192               0x1224ef1b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk192_hv128        0x1224efcc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x12205f6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_hk576_hv512        0x1220608a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h64             0x12205f9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h64       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h64            0x1208f5650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h64            0x1208f6550 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h64            0x1224ee350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h64            0x1208f6070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h64            0x1208f5950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h96             0x1220610c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h96       (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h96            0x12205ff50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h96            0x122062f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h96            0x1208f6930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h96            0x1224f0320 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h96            0x1224f2060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x122063730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x1208f79f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x1208f83d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x1208f9850 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x1208f8990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x1208f9f80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h192            0x1224f1100 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h192      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h192           0x1208fb370 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h192           0x1228182f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h192           0x1220649b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h192           0x1224f3910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h192           0x1224f4280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk192_hv128      0x1208fa920 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk192_hv128 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk192_hv128      0x1208fbd30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk192_hv128      0x1208fae30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk192_hv128      0x1224f45b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk192_hv128      0x1208fc720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk192_hv128      0x1224f4a20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x1224f5080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x122065640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x1208fc250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x1208fea20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x1224f63c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x122064d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_hk576_hv512      0x122065cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_hk576_hv512 (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_hk576_hv512      0x122816950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_hk576_hv512      0x122817830 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_hk576_hv512      0x122067030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_hk576_hv512      0x122062460 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_hk576_hv512      0x1220660e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x1208ff7e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x126504240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x122067f50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x122067970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x122068d90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x122069640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x1224f6770 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x1228198f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x1208ffaa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x126505210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x1224f78f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x1224f8570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f32                           0x1224f89e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_0_f16                           0x1224f7e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f32                           0x12206a7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q4_1_f16                           0x126505de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f32                           0x122818e50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_0_f16                           0x122069e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f32                           0x126506a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q5_1_f16                           0x12206b020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f32                           0x12206b2e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_q8_0_f16                           0x1224f92f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x126506290 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x12206c4d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x1224f95b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x12206c790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x126508af0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_neg                                    0x12206d6f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_reglu                                  0x1224f9da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu                                  0x126508de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu                                 0x12206d9b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_swiglu_oai                             0x1224fa4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_erf                              0x12206e280 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_geglu_quick                            0x126509610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x1224fabe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mean                                   0x122819ce0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x12206eb00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x12650a270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x12650a850 | th_max = 1024 | th_width =   32\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.19 MiB\n",
      "create_memory: n_ctx = 4096 (padded)\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   768.00 MiB\n",
      "llama_kv_cache_unified: size =  768.00 MiB (  4096 cells,  24 layers,  1/1 seqs), K (f16):  384.00 MiB, V (f16):  384.00 MiB\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 3\n",
      "llama_context: max_nodes = 1744\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   288.01 MiB\n",
      "llama_context: graph nodes  = 846\n",
      "llama_context: graph splits = 338 (with bs=512), 1 (with bs=1)\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | ACCELERATE = 1 | REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.pre': 'smollm', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'llama.embedding_length': '2048', 'llama.vocab_size': '49152', 'llama.attention.head_count_kv': '32', 'general.size_label': '1.7B', 'llama.block_count': '24', 'general.quantization_version': '2', 'llama.rope.dimension_count': '64', 'general.license': 'apache-2.0', 'llama.feed_forward_length': '8192', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.basename': 'smollm2', 'llama.attention.head_count': '32', 'llama.context_length': '8192', 'general.file_type': '15', 'general.finetune': '8k-mix7-ep2', 'general.organization': 'Loubnabnl', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.bos_token_id': '1', 'general.type': 'model', 'general.version': 'v2', 'llama.rope.freq_base': '130000.000000', 'general.name': 'Smollm2 1.7B 8k Mix7 Ep2 v2'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|im_start|>\n",
      "llama_perf_context_print:        load time =     298.77 ms\n",
      "llama_perf_context_print: prompt eval time =     298.59 ms /    27 tokens (   11.06 ms per token,    90.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4320.84 ms /   199 runs   (   21.71 ms per token,    46.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    4667.91 ms /   226 tokens\n",
      "llama_perf_context_print:    graphs reused =        191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a world where time was currency, the rich could live forever and the poor were left with nothing but the tick-tock of their mortality. A young man named Jack was born with an unusual gift – he could manipulate time. With his powers, Jack could slow down or speed up time to suit his needs.\n",
      "\n",
      "One day, while working at a small clock repair shop, Jack's life changed forever when a wealthy businessman walked in and asked for his help fixing an antique pocket watch. To Jack's surprise, the watch was priceless and held a secret. It was created by a mysterious timekeeper who had imbued it with the ability to manipulate time itself.\n",
      "\n",
      "As Jack worked on the watch, he began to realize that he could use its power to change his life forever. He could speed up time when he needed it, slowing down the days until their arrival like a gentle breeze on a summer's day. And when his life became too slow, he could speed it up,\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"llama.cpp/models/smollm2-1.7b-instruct-q4_k_m.gguf\",\n",
    "    n_ctx=4096,\n",
    "    n_threads=8,\n",
    "    n_gpu_layers=0,\n",
    ")\n",
    "\n",
    "prompt = \"\"\"<|im_start|>system\n",
    "You are a creative storyteller.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "Write a creative story\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "output = llm(\n",
    "    prompt,\n",
    "    max_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    stop=[\"<|im_end|>\"],\n",
    ")\n",
    "\n",
    "print(output[\"choices\"][0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c5e67c",
   "metadata": {},
   "source": [
    "vLLM advanced parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe79f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(model=\"http://localhost:8000/v1\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat_completion(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    max_tokens=200,\n",
    "    top_p=0.95,\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# For direct text generation\n",
    "response = client.text_generation(\n",
    "    \"Write a creative story about space exploration\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    details=True,\n",
    ")\n",
    "print(response.generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e374b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"not-needed\")\n",
    "\n",
    "# Advanced parameters example\n",
    "response = client.chat.completions.create(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "    ],\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=200,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f4b91",
   "metadata": {},
   "source": [
    "Using vLLM's Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2f28106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-08 16:23:10 [utils.py:263] non-default args: {'block_size': 16, 'gpu_memory_utilization': 0.85, 'max_num_batched_tokens': 8192, 'max_num_seqs': 256, 'disable_log_stats': True, 'model': 'HuggingFaceTB/SmolLM2-360M-Instruct'}\n",
      "INFO 02-08 16:23:11 [model.py:530] Resolved architecture: LlamaForCausalLM\n",
      "INFO 02-08 16:23:11 [model.py:1545] Using max model len 8192\n",
      "INFO 02-08 16:23:11 [scheduler.py:229] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 02-08 16:23:11 [cpu.py:190] CPU backend prefers block_size is multiples of 32, otherwise the performance is not optimized.\n",
      "INFO 02-08 16:23:13 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:23:14 [core.py:97] Initializing a V1 LLM engine (v0.14.1) with config: model='HuggingFaceTB/SmolLM2-360M-Instruct', speculative_config=None, tokenizer='HuggingFaceTB/SmolLM2-360M-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, enable_return_routed_experts=False, kv_cache_dtype=auto, device_config=cpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False, enable_mfu_metrics=False, enable_mm_processor_stats=False, enable_logging_iteration_details=False), seed=0, served_model_name=HuggingFaceTB/SmolLM2-360M-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.DYNAMO_TRACE_ONCE: 2>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [8192], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'dce': True, 'size_asserts': False, 'nan_asserts': False, 'epilogue_fusion': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False, 'assume_32_bit_indexing': True}, 'local_cache_dir': None}\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:23:14 [cpu_worker.py:86] Warning: NUMA is not enabled in this build. `init_cpu_threads_env` has no effect to setup thread affinity.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:23:14 [parallel_state.py:1214] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://192.168.100.71:63348 backend=gloo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W208 16:23:20.158669000 ProcessGroupGloo.cpp:547] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:20 [parallel_state.py:1425] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank N/A\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:20 [cpu_model_runner.py:55] Starting to load model HuggingFaceTB/SmolLM2-360M-Instruct...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:21 [weight_utils.py:550] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.13it/s]\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:22 [default_loader.py:291] Loading weights took 0.89 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:22 [kv_cache_utils.py:1305] GPU KV cache size: 209,712 tokens\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:22 [kv_cache_utils.py:1310] Maximum concurrency for 8,192 tokens per request: 25.60x\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:23 [cpu_model_runner.py:65] Warming up model for the compilation...\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:28 [cpu_model_runner.py:75] Warming up done.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:28 [core.py:273] init engine (profile, create kv cache, warmup model) took 6.61 seconds\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m INFO 02-08 16:24:29 [vllm.py:630] Asynchronous scheduling is disabled.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m WARNING 02-08 16:24:29 [vllm.py:672] Inductor compilation was disabled by user settings, optimizations settings that are only active during inductor compilation will be ignored.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m WARNING 02-08 16:24:29 [cpu.py:190] CPU backend prefers block_size is multiples of 32, otherwise the performance is not optimized.\n",
      "\u001b[0;36m(EngineCore_DP0 pid=25878)\u001b[0;0m WARNING 02-08 16:24:29 [cpu.py:157] VLLM_CPU_KVCACHE_SPACE not set. Using 8.0 GiB for KV cache.\n",
      "INFO 02-08 16:24:29 [llm.py:347] Supported tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 428.65it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.69s/it, est. speed input: 2.37 toks/s, output: 49.69 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " idea for a young female detective character with empathy and compassion. Your story should include:\n",
      "1. A relatable protagonist with a unique backstory,\n",
      "2. A compelling crime mystery to solve,\n",
      "3. Engaging dialogue that showcases her emotional intelligence and interpersonal skills, and\n",
      "4. An exciting plot twist that will captivate readers' attention and encourage them to continue reading to find out what happens next in the story.\n",
      "INFO 02-08 16:24:33 [chat_utils.py:599] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 3486.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.26s/it, est. speed input: 19.93 toks/s, output: 47.04 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a small, seaside town lived a curious cat named Luna. Luna was known for her incredible ability to solve mysteries and figure out puzzles that others found impossible. One sunny day, while wandering around the beach, she stumbled upon an old wooden box hidden beneath a pile of sand.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Initialize the model with advanced parameters\n",
    "llm = LLM(\n",
    "    model=\"HuggingFaceTB/SmolLM2-360M-Instruct\",\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_num_batched_tokens=8192,\n",
    "    max_num_seqs=256,\n",
    "    block_size=16,\n",
    ")\n",
    "\n",
    "# Configure sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,  # Higher for more creativity\n",
    "    top_p=0.95,  # Consider top 95% probability mass\n",
    "    max_tokens=100,  # Maximum length\n",
    "    presence_penalty=1.1,  # Reduce repetition\n",
    "    frequency_penalty=1.1,  # Reduce repetition\n",
    "    stop=[\"\\n\\n\", \"###\"],  # Stop sequences\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "prompt = \"Write a creative story\"\n",
    "outputs = llm.generate(prompt, sampling_params)\n",
    "print(outputs[0].outputs[0].text)\n",
    "\n",
    "# For chat-style interactions\n",
    "chat_prompt = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a creative storyteller.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Write a creative story\"},\n",
    "]\n",
    "formatted_prompt = llm.preprocess_chat(chat_prompt)  # Uses model's chat template\n",
    "outputs = llm.generate(formatted_prompt, sampling_params)\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9676b038",
   "metadata": {},
   "source": [
    "## Advanced Generation Control\n",
    "\n",
    "### Token Selection & Sampling\n",
    "\n",
    "TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de828fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.generate(\n",
    "    \"Write a creative story\",\n",
    "    temperature=0.8,  # Higher for more creativity\n",
    "    top_p=0.95,  # Consider top 95% probability mass\n",
    "    top_k=50,  # Consider top 50 tokens\n",
    "    max_new_tokens=100,  # Maximum length\n",
    "    repetition_penalty=1.1,  # Reduce repetition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b65e058",
   "metadata": {},
   "source": [
    "llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via OpenAI API compatibility\n",
    "response = client.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",  # Model name (can be any string for llama.cpp server)\n",
    "    prompt=\"Write a creative story\",\n",
    "    temperature=0.8,  # Higher for more creativity\n",
    "    top_p=0.95,  # Consider top 95% probability mass\n",
    "    frequency_penalty=1.1,  # Reduce repetition\n",
    "    presence_penalty=0.1,  # Reduce repetition\n",
    "    max_tokens=100,  # Maximum length\n",
    ")\n",
    "\n",
    "# Via llama-cpp-python direct access\n",
    "output = llm(\n",
    "    \"Write a creative story\",\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    max_tokens=100,\n",
    "    repeat_penalty=1.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ab6ec9",
   "metadata": {},
   "source": [
    "vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37759cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(\n",
    "    temperature=0.8,  # Higher for more creativity\n",
    "    top_p=0.95,  # Consider top 95% probability mass\n",
    "    top_k=50,  # Consider top 50 tokens\n",
    "    max_tokens=100,  # Maximum length\n",
    "    presence_penalty=0.1,  # Reduce repetition\n",
    ")\n",
    "llm.generate(\"Write a creative story\", sampling_params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab99564",
   "metadata": {},
   "source": [
    "### Control Repetition\n",
    "\n",
    "TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dc0f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.generate(\n",
    "    \"Write a varied text\",\n",
    "    repetition_penalty=1.1,  # Penalize repeated tokens\n",
    "    no_repeat_ngram_size=3,  # Prevent 3-gram repetition\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da3b0d",
   "metadata": {},
   "source": [
    "llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8757dbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via OpenAI API\n",
    "response = client.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    prompt=\"Write a varied text\",\n",
    "    frequency_penalty=1.1,  # Penalize frequent tokens\n",
    "    presence_penalty=0.8,  # Penalize tokens already present\n",
    ")\n",
    "\n",
    "# Via direct library\n",
    "output = llm(\n",
    "    \"Write a varied text\",\n",
    "    repeat_penalty=1.1,  # Penalize repeated tokens\n",
    "    frequency_penalty=0.5,  # Additional frequency penalty\n",
    "    presence_penalty=0.5,  # Additional presence penalty\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3aa646",
   "metadata": {},
   "source": [
    "vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf33ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(\n",
    "    presence_penalty=0.1,  # Penalize token presence\n",
    "    frequency_penalty=0.1,  # Penalize token frequency\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8b3abf",
   "metadata": {},
   "source": [
    "### Length Control and Stop Sequences\n",
    "\n",
    "TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a74311",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.generate(\n",
    "    \"Generate a short paragraph\",\n",
    "    max_new_tokens=100,\n",
    "    min_new_tokens=10,\n",
    "    stop_sequences=[\"\\n\\n\", \"###\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96c8da6",
   "metadata": {},
   "source": [
    "llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e5f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Via OpenAI API\n",
    "response = client.completions.create(\n",
    "    model=\"smollm2-1.7b-instruct\",\n",
    "    prompt=\"Generate a short paragraph\",\n",
    "    max_tokens=100,\n",
    "    stop=[\"\\n\\n\", \"###\"],\n",
    ")\n",
    "\n",
    "# Via direct library\n",
    "output = llm(\"Generate a short paragraph\", max_tokens=100, stop=[\"\\n\\n\", \"###\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846fc6b9",
   "metadata": {},
   "source": [
    "vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae156e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = SamplingParams(\n",
    "    max_tokens=100,\n",
    "    min_tokens=10,\n",
    "    stop=[\"###\", \"\\n\\n\"],\n",
    "    ignore_eos=False,\n",
    "    skip_special_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec05d8b",
   "metadata": {},
   "source": [
    "### Memory Management\n",
    "\n",
    "TGI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b2e431",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Docker deployment with memory optimization\n",
    "docker run --gpus all -p 8080:80 \\\n",
    "    --shm-size 1g \\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\n",
    "    --model-id HuggingFaceTB/SmolLM2-1.7B-Instruct \\\n",
    "    --max-batch-total-tokens 8192 \\\n",
    "    --max-input-length 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea724a",
   "metadata": {},
   "source": [
    "llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2aac6b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Server with memory optimizations\n",
    "./server \\\n",
    "    -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8080 \\\n",
    "    -c 2048 \\               # Context size\n",
    "    --threads 4 \\           # CPU threads\n",
    "    --n-gpu-layers 32 \\     # Use more GPU layers for larger models\n",
    "    --mlock \\               # Lock memory to prevent swapping\n",
    "    --cont-batching         # Enable continuous batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7cf9b4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "./server \\\n",
    "    -m smollm2-1.7b-instruct.Q4_K_M.gguf \\\n",
    "    --n-gpu-layers 20 \\     # Keep first 20 layers on GPU\n",
    "    --threads 8             # Use more CPU threads for CPU layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79085fd0",
   "metadata": {},
   "source": [
    "vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07484567",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm.engine.arg_utils import AsyncEngineArgs\n",
    "\n",
    "engine_args = AsyncEngineArgs(\n",
    "    model=\"HuggingFaceTB/SmolLM2-1.7B-Instruct\",\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_num_batched_tokens=8192,\n",
    "    block_size=16,\n",
    ")\n",
    "\n",
    "llm = LLM(engine_args=engine_args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
